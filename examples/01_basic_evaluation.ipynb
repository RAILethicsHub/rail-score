{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Evaluation with RAIL Score\n",
    "\n",
    "This notebook demonstrates basic content evaluation using the RAIL Score Python SDK.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to initialize the RAIL Score client\n",
    "- Perform basic evaluations across all 8 dimensions\n",
    "- Understand the response structure\n",
    "- Interpret scores and confidence levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the RAIL Score SDK if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install rail-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rail_score import RailScore\n",
    "\n",
    "# Initialize client\n",
    "# Option 1: Use environment variable (recommended)\n",
    "client = RailScore(api_key=os.getenv(\"RAIL_API_KEY\"))\n",
    "\n",
    "# Option 2: Direct API key (not recommended for production)\n",
    "# client = RailScore(api_key=\"your-api-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Evaluation\n",
    "\n",
    "Let's evaluate a simple AI-generated statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content to evaluate\n",
    "content = \"Our AI system prioritizes user privacy and data security through encryption and access controls.\"\n",
    "\n",
    "# Perform evaluation\n",
    "result = client.evaluation.basic(content)\n",
    "\n",
    "# Display overall score\n",
    "print(f\"Overall RAIL Score: {result.rail_score.score:.2f}/10\")\n",
    "print(f\"Confidence: {result.rail_score.confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Exploring Dimension Scores\n",
    "\n",
    "Let's look at the individual dimension scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDimension Scores:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dim_name, dim_score in result.scores.items():\n",
    "    print(f\"\\n{dim_name.upper().replace('_', ' ')}\")\n",
    "    print(f\"  Score: {dim_score.score:.2f}/10\")\n",
    "    print(f\"  Confidence: {dim_score.confidence:.2%}\")\n",
    "    print(f\"  Explanation: {dim_score.explanation}\")\n",
    "    \n",
    "    if dim_score.issues:\n",
    "        print(f\"  Issues: {', '.join(dim_score.issues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Visualizing Scores\n",
    "\n",
    "Let's create a bar chart to visualize the dimension scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract dimension names and scores\n",
    "dimensions = [dim.replace('_', ' ').title() for dim in result.scores.keys()]\n",
    "scores = [dim_score.score for dim_score in result.scores.values()]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(dimensions, scores, color='steelblue')\n",
    "\n",
    "# Add a horizontal line at the overall score\n",
    "plt.axhline(y=result.rail_score.score, color='red', linestyle='--', \n",
    "            label=f'Overall Score: {result.rail_score.score:.2f}')\n",
    "\n",
    "# Customize chart\n",
    "plt.xlabel('RAIL Dimensions')\n",
    "plt.ylabel('Score (0-10)')\n",
    "plt.title('RAIL Score Evaluation by Dimension')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 10)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Comparing Multiple Contents\n",
    "\n",
    "Let's evaluate and compare multiple AI-generated statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [\n",
    "    \"Our AI makes automated decisions without human oversight.\",\n",
    "    \"We collect user data to improve our services with explicit consent.\",\n",
    "    \"The AI model is a black box - we don't know how it works.\",\n",
    "    \"Our system provides clear explanations for all decisions and protects user privacy.\"\n",
    "]\n",
    "\n",
    "# Evaluate each content\n",
    "results = []\n",
    "for i, content in enumerate(contents, 1):\n",
    "    result = client.evaluation.basic(content)\n",
    "    results.append(result)\n",
    "    print(f\"Content {i}: {result.rail_score.score:.2f}/10\")\n",
    "    print(f\"  Text: {content[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Understanding Metadata\n",
    "\n",
    "Each evaluation response includes useful metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from the last result\n",
    "metadata = results[-1].metadata\n",
    "\n",
    "print(\"Request Metadata:\")\n",
    "print(f\"  Request ID: {metadata.req_id}\")\n",
    "print(f\"  Plan Tier: {metadata.tier}\")\n",
    "print(f\"  Credits Consumed: {metadata.credits_consumed}\")\n",
    "print(f\"  Processing Time: {metadata.processing_time_ms:.0f}ms\")\n",
    "print(f\"  Queue Wait Time: {metadata.queue_wait_time_ms:.0f}ms\")\n",
    "print(f\"  Timestamp: {metadata.timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Custom Weights\n",
    "\n",
    "You can apply custom importance weights to different dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom weights (must sum to 100)\n",
    "custom_weights = {\n",
    "    \"safety\": 35,\n",
    "    \"privacy\": 30,\n",
    "    \"reliability\": 15,\n",
    "    \"accountability\": 10,\n",
    "    \"transparency\": 5,\n",
    "    \"fairness\": 3,\n",
    "    \"inclusivity\": 1,\n",
    "    \"user_impact\": 1\n",
    "}\n",
    "\n",
    "# Evaluate with custom weights\n",
    "weighted_result = client.evaluation.basic(\n",
    "    \"Healthcare AI system for patient diagnosis\",\n",
    "    weights=custom_weights\n",
    ")\n",
    "\n",
    "print(f\"Weighted RAIL Score: {weighted_result.rail_score.score:.2f}/10\")\n",
    "print(\"\\nTop 3 Dimensions (by weight):\")\n",
    "for dim in [\"safety\", \"privacy\", \"reliability\"]:\n",
    "    score = weighted_result.scores[dim]\n",
    "    print(f\"  {dim.title()}: {score.score:.2f}/10 (weight: {custom_weights[dim]}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Error Handling\n",
    "\n",
    "Here's how to handle potential errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail_score import (\n",
    "    AuthenticationError,\n",
    "    InsufficientCreditsError,\n",
    "    ValidationError,\n",
    "    RateLimitError\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = client.evaluation.basic(\"Test content\")\n",
    "    print(f\"✅ Evaluation successful: {result.rail_score.score:.2f}/10\")\n",
    "    \n",
    "except AuthenticationError:\n",
    "    print(\"❌ Authentication failed. Check your API key.\")\n",
    "    \n",
    "except InsufficientCreditsError as e:\n",
    "    print(f\"❌ Insufficient credits. Balance: {e.balance}, Required: {e.required}\")\n",
    "    \n",
    "except ValidationError as e:\n",
    "    print(f\"❌ Validation error: {e}\")\n",
    "    \n",
    "except RateLimitError as e:\n",
    "    print(f\"❌ Rate limit exceeded. Retry after {e.retry_after}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand basic evaluation, explore:\n",
    "\n",
    "- **02_dimension_specific.ipynb** - Focus on specific dimensions\n",
    "- **03_batch_processing.ipynb** - Evaluate multiple items efficiently\n",
    "- **04_compliance_checks.ipynb** - Check GDPR, HIPAA, CCPA compliance\n",
    "- **05_rag_evaluation.ipynb** - Detect hallucinations in RAG systems\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [API Reference](https://responsibleailabs.ai/docs/api-reference)\n",
    "- [Full Documentation](https://responsibleailabs.ai/docs)\n",
    "- [GitHub Repository](https://github.com/Responsible-AI-Labs/rail-score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
